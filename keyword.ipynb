{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser as fp\n",
    "import dateutil.parser\n",
    "from newspaper import Article, Config\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import os\n",
    "import flask\n",
    "import webbrowser\n",
    "import random\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class Helper:\n",
    "    @staticmethod\n",
    "    def print_scrape_status(count):\n",
    "        logging.info(f'Scraped {count} articles so far...')\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_dataframe(news_df):\n",
    "        # Function that cleans the article dataframe\n",
    "        news_df = news_df[news_df.title != '']\n",
    "        news_df = news_df[news_df.body != '']\n",
    "        news_df = news_df[news_df.image_url != '']\n",
    "        news_df = news_df[news_df.title.str.count('\\s+').ge(3)]  # keep only titles having more than 3 spaces in the title\n",
    "        news_df = news_df[news_df.body.str.count('\\s+').ge(20)]  # keep only titles having more than 20 spaces in the body\n",
    "        return news_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_articles(news_df):\n",
    "        # Function that cleans all the bodies of the articles\n",
    "        # Drop Duplicates\n",
    "        news_df = (news_df.drop_duplicates(subset=[\"title\", \"source\"])).sort_index()\n",
    "        news_df = (news_df.drop_duplicates(subset=[\"body\"])).sort_index()\n",
    "        news_df = (news_df.drop_duplicates(subset=[\"url\"])).sort_index()\n",
    "        news_df = news_df.reset_index(drop=True)\n",
    "        \n",
    "        # Make all letters lower case\n",
    "        news_df['clean_body'] = news_df['body'].str.lower()\n",
    "        \n",
    "        # Filter out the stopwords and punctuation\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "        \n",
    "        # Remove digits\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "        \n",
    "        # Log contents of clean_body after removing digits\n",
    "        logging.info(\"Contents of 'clean_body' after removing digits:\")\n",
    "        for i, body in enumerate(news_df['clean_body'].head(10)):\n",
    "            logging.info(f'Article {i + 1}: {body}')\n",
    "        \n",
    "        # Remove sources\n",
    "        sources_set = [x.lower() for x in set(news_df['source'])]\n",
    "        sources_to_replace = dict.fromkeys(sources_set, \"\")\n",
    "        news_df['clean_body'] = (news_df['clean_body'].replace(sources_to_replace, regex=True))\n",
    "        \n",
    "        # Unidecode all characters\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(unidecode)\n",
    "        \n",
    "        # Tokenize\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(word_tokenize)\n",
    "        \n",
    "        # Stem words\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "        news_df['clean_body'] = news_df['clean_body'].apply(lambda x: ' '.join([word for word in x]))\n",
    "        \n",
    "        # Debugging: Print clean_body column to check for digits\n",
    "        logging.info(\"Contents of 'clean_body' after cleaning:\")\n",
    "        for i, body in enumerate(news_df['clean_body'].head(10)):\n",
    "            logging.info(f'Article {i + 1}: {body}')\n",
    "        \n",
    "        return news_df\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle_content(clusters_dict):\n",
    "        # Shuffle the content within each cluster\n",
    "        for cluster in clusters_dict.values():\n",
    "            random.shuffle(cluster)\n",
    "\n",
    "    @staticmethod\n",
    "    def prettify_similar(clusters_dict):\n",
    "        # Create a list of similar articles for display\n",
    "        similar_articles = []\n",
    "        for cluster in clusters_dict.values():\n",
    "            cluster_titles = [article['title'] for article in cluster]\n",
    "            similar_articles.append(', '.join(cluster_titles))\n",
    "        return similar_articles\n",
    "\n",
    "class CacheManager:\n",
    "    def __init__(self, cache_file='article_cache.json'):\n",
    "        self.cache_file = cache_file\n",
    "        self.load_cache()\n",
    "    \n",
    "    def load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                self.cache = json.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(self.cache, f, indent=4)\n",
    "    \n",
    "    def get_article(self, url, keyword):\n",
    "        cached_article = self.cache.get(url, None)\n",
    "        if cached_article and cached_article.get('contains_keyword', False):\n",
    "            return cached_article\n",
    "        return None\n",
    "    \n",
    "    def add_article(self, url, article_data, keyword):\n",
    "        # Check if keyword is present in title or body\n",
    "        contains_keyword = (keyword in article_data['title'].lower() or keyword in article_data['body'].lower())\n",
    "        article_data['contains_keyword'] = contains_keyword\n",
    "        self.cache[url] = article_data\n",
    "        self.save_cache()\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, sources, days, keyword, cache_manager):\n",
    "        self.sources = sources\n",
    "        self.days = days\n",
    "        self.keyword = keyword.lower()\n",
    "        self.cache_manager = cache_manager\n",
    "    \n",
    "    def scrape(self):\n",
    "        try:\n",
    "            articles_list = []\n",
    "            now = datetime.now(timezone.utc)\n",
    "            total_articles_scraped = 0  # Track total number of articles scraped\n",
    "            \n",
    "            for source, content in self.sources.items():\n",
    "                logging.info(f'Source: {source}')\n",
    "                logging.info(f'Content: {content}')\n",
    "                for url in content['rss']:\n",
    "                    logging.info(f'Processing RSS feed: {url}')\n",
    "                    try:\n",
    "                        d = fp.parse(url)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f'Error parsing RSS feed {url}: {e}')\n",
    "                        continue\n",
    "                    \n",
    "                    for entry in d.entries:\n",
    "                        if not hasattr(entry, 'published'):\n",
    "                            logging.warning(f'Entry missing \"published\" attribute: {entry}')\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            article_date = dateutil.parser.parse(getattr(entry, 'published'))\n",
    "                            article_date = article_date.astimezone(timezone.utc)  # Ensure article_date is offset-aware\n",
    "                            logging.info(f'Found article with date: {article_date}')\n",
    "                        except Exception as e:\n",
    "                            logging.error(f'Error parsing article date: {e}')\n",
    "                            continue\n",
    "                        \n",
    "                        if now - article_date <= timedelta(days=self.days):\n",
    "                            try:\n",
    "                                logging.info(f'Processing article: {entry.link}')\n",
    "                                content = Article(entry.link, config=config)\n",
    "                                content.download()\n",
    "                                content.parse()\n",
    "                                content.nlp()\n",
    "                                \n",
    "                                article = {\n",
    "                                    'source': source,\n",
    "                                    'url': entry.link,\n",
    "                                    'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                    'time': article_date.strftime('%H:%M:%S %Z'),\n",
    "                                    'title': content.title,\n",
    "                                    'body': content.text,\n",
    "                                    'summary': content.summary,\n",
    "                                    'keywords': content.keywords,\n",
    "                                    'image_url': content.top_image,\n",
    "                                }\n",
    "                                \n",
    "                                # Add article to cache\n",
    "                                self.cache_manager.add_article(entry.link, article, self.keyword)\n",
    "                                \n",
    "                                # Add to list of all scraped articles\n",
    "                                articles_list.append(article)\n",
    "                                total_articles_scraped += 1\n",
    "                                \n",
    "                                # Log status of scraping\n",
    "                                Helper.print_scrape_status(total_articles_scraped)\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                logging.error(f'Error processing article: {e}')\n",
    "                                logging.info('Continuing...')\n",
    "                        else:\n",
    "                            logging.info(f'Skipping article outside date range: {entry.link}')\n",
    "            \n",
    "            logging.info(f'Total articles scraped: {total_articles_scraped}')\n",
    "            return articles_list\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f'Error in \"Scraper.scrape()\": {e}')\n",
    "            raise Exception(f'Error in \"Scraper.scrape()\": {e}')\n",
    "\n",
    "def compute_tfidf(news_df):\n",
    "    # Function that computes the TFIDF values for all words in the article bodies\n",
    "    tfidf_matrix = TfidfVectorizer().fit_transform(news_df['clean_body'])\n",
    "    tfidf_array = np.asarray(tfidf_matrix.todense())  # Convert to numpy array\n",
    "    return tfidf_array\n",
    "\n",
    "def find_featured_clusters(clusters):\n",
    "    # Function that finds clusters with articles from multiple sources\n",
    "    featured_clusters = {}\n",
    "    for i in clusters.keys():\n",
    "        if len(set([j[\"source\"] for j in clusters[i]])) > 1:\n",
    "            featured_clusters[i] = clusters[i]\n",
    "    return featured_clusters\n",
    "\n",
    "def build_html(clusters_dict, news_name, news_date, template_string, output_filename):\n",
    "    app = flask.Flask(__name__)\n",
    "\n",
    "    with app.app_context():\n",
    "        rendered = flask.render_template_string(template_string, \n",
    "                                                news_name=news_name, \n",
    "                                                news_date=news_date, \n",
    "                                                clusters_dict=clusters_dict)\n",
    "\n",
    "    with open(output_filename, 'w', encoding=\"utf-8\") as output:\n",
    "        output.write(rendered)\n",
    "\n",
    "    webbrowser.open_new_tab(output_filename)\n",
    "\n",
    "    return True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('sources.json', 'r') as file:\n",
    "        sources = json.load(file)\n",
    "    \n",
    "    news_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    days_back = 7  \n",
    "    \n",
    "    keyword = \"Debate\"  \n",
    "    \n",
    "    cache_manager = CacheManager()\n",
    "    \n",
    "    scraper = Scraper(sources, days_back, keyword, cache_manager)\n",
    "    try:\n",
    "        articles = scraper.scrape()\n",
    "        \n",
    "        if not articles:\n",
    "            logging.warning('No articles were scraped.')\n",
    "        else:\n",
    "            news_df = pd.DataFrame(articles)\n",
    "            news_df = Helper.clean_dataframe(news_df)\n",
    "            \n",
    "            # Log number of articles being processed\n",
    "            logging.info(f'Processing {len(news_df)} articles...')\n",
    "            \n",
    "            # Filter articles that contain the keyword\n",
    "            news_df['contains_keyword'] = news_df['body'].str.lower().str.contains(keyword.lower())\n",
    "            filtered_news_df = news_df[news_df['contains_keyword']]\n",
    "            \n",
    "            news_df = Helper.clean_articles(news_df)\n",
    "            \n",
    "            news_df.to_csv('cleaned_articles.csv', index=False)\n",
    "\n",
    "            tfidf_df = compute_tfidf(news_df)\n",
    "            \n",
    "            distance_threshold = 1\n",
    "            ac = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None).fit(tfidf_df)\n",
    "            articles_labeled = ac.fit_predict(tfidf_df)\n",
    "            \n",
    "            clusters = {n: news_df.iloc[np.where(articles_labeled == n)].to_dict(orient='records') for n in np.unique(articles_labeled)}\n",
    "            \n",
    "            featured_clusters = find_featured_clusters(clusters)\n",
    "            \n",
    "            template_filename = 'template.html'\n",
    "            output_filename = 'newsletter.html'\n",
    "            \n",
    "            with open(template_filename, 'r', encoding='utf-8') as template_file:\n",
    "                template_string = template_file.read()\n",
    "            \n",
    "            build_html(featured_clusters, \"Daily News - A Enterprises for big data project\", news_date, template_string, output_filename)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f'An error occurred: {e}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
